{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"V28","mount_file_id":"1PezO8xRdzrtV83ncO9_ajLvjsyzTnZrP","authorship_tag":"ABX9TyNcOne0lZoAarvHILZf+iav"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU","widgets":{"application/vnd.jupyter.widget-state+json":{"d8039c9299744559b3f64ba81feb8bf9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3fc7fa4c1ddd4a91bcc9019af02f40e4","IPY_MODEL_c6621760e2f24916a58c8541395292dc","IPY_MODEL_c1e4bbecd8c643f090e221f0c4723c9a"],"layout":"IPY_MODEL_c3983c518d3b4d7ead89f5bd7a12d106"}},"3fc7fa4c1ddd4a91bcc9019af02f40e4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_872114ab5e8141cfaf9017cacb3ca3e0","placeholder":"​","style":"IPY_MODEL_c1ee8100cad447b49593c7901d15cfd0","value":"tokenizer_config.json: 100%"}},"c6621760e2f24916a58c8541395292dc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_66d39d88ee20442c8f6f81768526b912","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ba87e0c2fa2540afa6208328db19e0f9","value":25}},"c1e4bbecd8c643f090e221f0c4723c9a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_41be995f4b64452cb706fecdb72082d0","placeholder":"​","style":"IPY_MODEL_2ec2b8733eaa486c83e96a0b0b15a29d","value":" 25.0/25.0 [00:00&lt;00:00, 3.07kB/s]"}},"c3983c518d3b4d7ead89f5bd7a12d106":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"872114ab5e8141cfaf9017cacb3ca3e0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c1ee8100cad447b49593c7901d15cfd0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"66d39d88ee20442c8f6f81768526b912":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ba87e0c2fa2540afa6208328db19e0f9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"41be995f4b64452cb706fecdb72082d0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2ec2b8733eaa486c83e96a0b0b15a29d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"31e3a7ec8d4843c9ba0bc86298e2a67f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_41f31fd356fe41a19893a94cea5b8da6","IPY_MODEL_c051656f7a2d4f818f60634a073b3c1d","IPY_MODEL_ee15f4198a4c4a68a2610e3f9faba437"],"layout":"IPY_MODEL_ae09b682875742fdb4f7de86a5856900"}},"41f31fd356fe41a19893a94cea5b8da6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d62eab05c8434ffebfd9dbba9ed85052","placeholder":"​","style":"IPY_MODEL_3c78b37bcc324a4c8af17bcebff9dfb5","value":"config.json: 100%"}},"c051656f7a2d4f818f60634a073b3c1d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_31c37c2ac3a9429ea808c7ae2d142dfb","max":482,"min":0,"orientation":"horizontal","style":"IPY_MODEL_903b2f0c21b547af9a5d870f876379bc","value":482}},"ee15f4198a4c4a68a2610e3f9faba437":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_142b18d7504346208438dbc26c7840d1","placeholder":"​","style":"IPY_MODEL_a6c2360d642a4edbbc4e72c2a8b832d4","value":" 482/482 [00:00&lt;00:00, 70.3kB/s]"}},"ae09b682875742fdb4f7de86a5856900":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d62eab05c8434ffebfd9dbba9ed85052":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3c78b37bcc324a4c8af17bcebff9dfb5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"31c37c2ac3a9429ea808c7ae2d142dfb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"903b2f0c21b547af9a5d870f876379bc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"142b18d7504346208438dbc26c7840d1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a6c2360d642a4edbbc4e72c2a8b832d4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2d201e38fbd54672b7cf789216ae2f55":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ac29c0ed146d456880bb442281f19b00","IPY_MODEL_224e08ec7b684e8d88bc437f44dc55ce","IPY_MODEL_acf250e0f5414e4bbd05d405f1f682b4"],"layout":"IPY_MODEL_dd70ff0ae2544637b01c3fe340ae9518"}},"ac29c0ed146d456880bb442281f19b00":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4948e962cd0440bdb330be424871f357","placeholder":"​","style":"IPY_MODEL_7774c95fe0eb434faf43ab79542c521c","value":"vocab.json: 100%"}},"224e08ec7b684e8d88bc437f44dc55ce":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8bd66190bee44bc6b19e2241c1cdfd06","max":898823,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0630619c850442b89bf7ae3b193df57d","value":898823}},"acf250e0f5414e4bbd05d405f1f682b4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_efb0a722612c4f04870801cf9b0ddde4","placeholder":"​","style":"IPY_MODEL_24963ef522db4285b15c984f70bddb36","value":" 899k/899k [00:00&lt;00:00, 13.2MB/s]"}},"dd70ff0ae2544637b01c3fe340ae9518":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4948e962cd0440bdb330be424871f357":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7774c95fe0eb434faf43ab79542c521c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8bd66190bee44bc6b19e2241c1cdfd06":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0630619c850442b89bf7ae3b193df57d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"efb0a722612c4f04870801cf9b0ddde4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"24963ef522db4285b15c984f70bddb36":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"30bc0e516bfa4058ac8297972115ff7d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0bdb48c93f4f4d43a1066d2ee681cead","IPY_MODEL_dd626b2e8f134c9eb875391ae1cc6660","IPY_MODEL_3b80cfed981b418d801d0d7cbab622f1"],"layout":"IPY_MODEL_c335f31aec3b42e99a7bc65836eb21cb"}},"0bdb48c93f4f4d43a1066d2ee681cead":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_529c07ba615e4d29a0a3421411861159","placeholder":"​","style":"IPY_MODEL_843f393dff0743498087c328f2abdf9e","value":"merges.txt: 100%"}},"dd626b2e8f134c9eb875391ae1cc6660":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_574ee72979e54c3a995bd2071bbc1efe","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2c74646a34f14b20beb66e52cd0b5b9e","value":456318}},"3b80cfed981b418d801d0d7cbab622f1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_506882a3055344b9900d6a0d2354ad6e","placeholder":"​","style":"IPY_MODEL_4161377037294890be40c654f2b14ea0","value":" 456k/456k [00:00&lt;00:00, 50.0MB/s]"}},"c335f31aec3b42e99a7bc65836eb21cb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"529c07ba615e4d29a0a3421411861159":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"843f393dff0743498087c328f2abdf9e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"574ee72979e54c3a995bd2071bbc1efe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2c74646a34f14b20beb66e52cd0b5b9e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"506882a3055344b9900d6a0d2354ad6e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4161377037294890be40c654f2b14ea0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9496bec3764c46ac9257c272f1733a20":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_940dbe82a5a542a5973cbebe02a1b500","IPY_MODEL_7edb61000ee14853b390f471190bc417","IPY_MODEL_251fa0f9c75a41f997a7ecdbafbcb087"],"layout":"IPY_MODEL_9d1f99fe17fb476c96b344f39a38b0fc"}},"940dbe82a5a542a5973cbebe02a1b500":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_197443e0b87c47b09e917a281182562d","placeholder":"​","style":"IPY_MODEL_75612a3c74c84c2d89bab7695bcfe88f","value":"tokenizer.json: 100%"}},"7edb61000ee14853b390f471190bc417":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5fd5dda304464697952e9891aaa94c55","max":1355863,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b1a9420ce80140d29c99b6828d54556b","value":1355863}},"251fa0f9c75a41f997a7ecdbafbcb087":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_463d0523782a4b0d945b2343e6112125","placeholder":"​","style":"IPY_MODEL_3e0cd5d45d3e430db48265efd252cc51","value":" 1.36M/1.36M [00:00&lt;00:00, 13.3MB/s]"}},"9d1f99fe17fb476c96b344f39a38b0fc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"197443e0b87c47b09e917a281182562d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"75612a3c74c84c2d89bab7695bcfe88f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5fd5dda304464697952e9891aaa94c55":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b1a9420ce80140d29c99b6828d54556b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"463d0523782a4b0d945b2343e6112125":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3e0cd5d45d3e430db48265efd252cc51":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4140730bcb0f479f96850c85e1d8ce5d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_607bac5033794c70bbd3dcff015a8924","IPY_MODEL_97de5c649b61436caf59a9ddfd7f16f1","IPY_MODEL_ec4fd8a08a5f4e55908d159ad272d753"],"layout":"IPY_MODEL_a625edfebe7245e891e539f4e08e39f0"}},"607bac5033794c70bbd3dcff015a8924":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_81168e222bb5496c966c3e9d7708e873","placeholder":"​","style":"IPY_MODEL_a24428797ca541e4a86d68a8cdf12bce","value":"model.safetensors: 100%"}},"97de5c649b61436caf59a9ddfd7f16f1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2ffecd15eb1d45e5b4192bc73a1d72cf","max":1421700479,"min":0,"orientation":"horizontal","style":"IPY_MODEL_69fa9475e4d34ea9becdd231304aa058","value":1421700479}},"ec4fd8a08a5f4e55908d159ad272d753":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1830da3095ec435b9a44b460fce8dafd","placeholder":"​","style":"IPY_MODEL_aa2fa65fadf54425b293b06445c83247","value":" 1.42G/1.42G [00:05&lt;00:00, 283MB/s]"}},"a625edfebe7245e891e539f4e08e39f0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"81168e222bb5496c966c3e9d7708e873":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a24428797ca541e4a86d68a8cdf12bce":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2ffecd15eb1d45e5b4192bc73a1d72cf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"69fa9475e4d34ea9becdd231304aa058":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1830da3095ec435b9a44b460fce8dafd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aa2fa65fadf54425b293b06445c83247":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["# Install necessary packages\n","!pip install rouge pandas transformers datasets nltk rouge_score bert_score moverscore pyemd pytorch_pretrained_bert evaluate"],"metadata":{"id":"iWzEOCYnCzLS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install --upgrade numpy==1.26.0"],"metadata":{"id":"UVFy8NKIQnU-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from rouge import Rouge\n","import torch\n","from tqdm import tqdm\n","import re\n","from sklearn.model_selection import train_test_split\n","from datasets import Dataset\n","from rouge_score import rouge_scorer\n","import evaluate"],"metadata":{"id":"I4TR2TUmCzFT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import pickle\n","from collections import Counter\n","\n","from typing import Callable, Optional\n","from copy import deepcopy\n","\n","# Import pytoch libraries and modules\n","import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","from torch import optim\n","from torch.nn.utils import clip_grad_norm_\n","from torch.utils.data import Dataset, DataLoader\n","from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n","\n","#Import libraries for text procesing\n","import nltk\n","from nltk.tokenize import word_tokenize\n","nltk.download('punkt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"is6BK8dDEQhc","executionInfo":{"status":"ok","timestamp":1744808494807,"user_tz":-330,"elapsed":6914,"user":{"displayName":"Anindita Bhattacharya","userId":"16794923920170422248"}},"outputId":"44f3c723-aaeb-46b0-c8fe-d05f6b81bf73"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["!pip install pkbar\n","import pkbar"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_J6Ru_RjEaFj","executionInfo":{"status":"ok","timestamp":1744808498424,"user_tz":-330,"elapsed":1587,"user":{"displayName":"Anindita Bhattacharya","userId":"16794923920170422248"}},"outputId":"7ec3bf25-8c01-45ec-8602-254345c79857"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pkbar\n","  Downloading pkbar-0.5-py3-none-any.whl.metadata (3.8 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pkbar) (1.26.0)\n","Downloading pkbar-0.5-py3-none-any.whl (9.2 kB)\n","Installing collected packages: pkbar\n","Successfully installed pkbar-0.5\n"]}]},{"cell_type":"code","source":["nltk.download('punkt_tab')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mVAbDMgfr_I4","executionInfo":{"status":"ok","timestamp":1744808513092,"user_tz":-330,"elapsed":443,"user":{"displayName":"Anindita Bhattacharya","userId":"16794923920170422248"}},"outputId":"a466c2ee-3329-42b4-a08e-ac2aef36e72c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["# Load the original train and test dataset\n","train_data = pd.read_csv(\"/content/drive/MyDrive/mimic_train.csv\")\n","test_data = pd.read_csv('/content/drive/MyDrive/mimic_test.csv')\n","\n","# Select first 3000 rows from the train dataset\n","train_data = train_data.iloc[:3000].copy()\n","# Select first 500 rows from the test dataset\n","test_data = test_data.iloc[:500].copy()\n","\n","# Split into train (80%) and validation (20%) sets\n","train_df, val_df = train_test_split(train_data, test_size=0.2, random_state=42)\n","\n","# Save the datasets to CSV files\n","train_df.to_csv(\"/content/drive/MyDrive/train_split.csv\", index=False)\n","val_df.to_csv(\"/content/drive/MyDrive/val_split.csv\", index=False)\n","test_data.to_csv(\"/content/drive/MyDrive/test_split.csv\", index=False)"],"metadata":{"id":"jZMByChkIV6f"},"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"Y-xzeRcNCfXc"},"cell_type":"code","source":["class Parameters:\n","  # Model Parameters\n","  hidden_size: int = 150  # of the encoder; default decoder size is doubled if encoder is bidi\n","  dec_hidden_size: Optional[int] = 200  # if set, a matrix will transform enc state into dec state\n","  embed_size: int = 100 # Size of the embedding vectors\n","  eps=1e-31\n","  batch_size=16\n","  enc_bidi = True #Set the encoder as bidirectional\n","  enc_rnn_dropout = 0.1 # Set the dropout parameter in the encoder\n","  enc_attn = True #Activate the encoder attention\n","  dec_attn = True #Activate the decoder attention\n","  pointer = True #Activate the pointer generator mechanism\n","  # Set different dropout probabilities in the decoder\n","  dec_in_dropout=0.1\n","  dec_rnn_dropout=0.1\n","  dec_out_dropout=0.1\n","  # Vocabulary and data parameters\n","  max_src_len: int = 65  # exclusive of special tokens such as EOS\n","  max_tgt_len: int = 15  # exclusive of special tokens such as EOS\n","  vocab_min_frequency: int = 3\n","  # Data paths\n","  embed_file: Optional[str] = '/content/drive/MyDrive/glove.6B.100d.txt'  # use pre-trained embeddings\n","  train_data_path: str = '/content/drive/MyDrive/train_split.csv'\n","  val_data_path: Optional[str] = '/content/drive/MyDrive/val_split.csv'\n","  test_data_path: str = '/content/drive/MyDrive/test_split.csv'\n","  # Parameters to save the model\n","  resume_train = False\n","  encoder_weights_path='encoder_sum.pt'\n","  decoder_weights_path='decoder_sum.pt'\n","  encoder_decoder_adapter_weights_path='adapter_sum.pt'\n","  losses_path='val_losses.pkl'\n","  print_every = 100"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def simple_tokenizer(text, lower=False, newline=None):\n","  if lower:\n","    text = text.lower()\n","  if newline is not None:\n","    text = text.replace('\\n', ' ' + newline + ' ')\n","  return text.split()"],"metadata":{"id":"twW14sm1FQWo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Vocab(object):\n","  PAD = 0\n","  SOS = 1\n","  EOS = 2\n","  UNK = 3\n","\n","  def __init__(self):\n","    ''' Initialize the structures to store the information about the vocabulary'''\n","    self.word2index = {}\n","    self.word2count = Counter()\n","    self.reserved = ['<PAD>', '<SOS>', '<EOS>', '<UNK>']\n","    self.index2word = self.reserved[:]\n","    self.embeddings = None\n","\n","  def add_words(self, words):\n","    ''' Add words to the vocabulary'''\n","    for word in words:\n","      #if it is an unseen word\n","      if word not in self.word2index:\n","        #Include the word in the mapping from word to index\n","        self.word2index[word] = len(self.index2word)\n","        # Include the word in the indexes\n","        self.index2word.append(word)\n","    # Increment the count of ocurrences of the word to 1\n","    self.word2count.update(words)\n","\n","  def load_embeddings(self, file_path: str, dtype=np.float32) -> int:\n","    ''' Load the embedding vectors from a file into the vocabulary'''\n","    num_embeddings = 0\n","    vocab_size = len(self)\n","    with open(file_path, 'rb') as f:\n","      # For every word in the embedding vectors\n","      for line in f:\n","        line = line.split()\n","        word = line[0].decode('utf-8')\n","        # Get the index of the embedded word\n","        idx = self.word2index.get(word)\n","        if idx is not None:\n","          # Extract the embedding vector of the word\n","          vec = np.array(line[1:], dtype=dtype)\n","          #If the embedding vector is not initialized\n","          if self.embeddings is None:\n","            # Set the embeddings dimension, initialize the embedding vector to zeros\n","            n_dims = len(vec)\n","            self.embeddings = np.random.normal(np.zeros((vocab_size, n_dims))).astype(dtype)\n","            self.embeddings[self.PAD] = np.zeros(n_dims)\n","          # Store the embedding in the array of embeddings\n","          self.embeddings[idx] = vec\n","          num_embeddings += 1\n","    return num_embeddings\n","\n","  def save_to_file(self, filename):\n","    ''' Save the Vocab object to a file'''\n","    with open(filename,'wb') as f:\n","        pickle.dump(self,f)\n","\n","  def __getitem__(self, item):\n","    ''' Get the next item when iterating over the instance'''\n","    if type(item) is int:\n","      return self.index2word[item]\n","    return self.word2index.get(item, self.UNK)\n","\n","  def __len__(self):\n","    ''' Return the length of the instance or vocabulary'''\n","    return len(self.index2word)\n","\n","\n","def load_vocab(filename):\n","    ''' Load a Vocab instance from a file'''\n","    with open(filename,'rb') as f:\n","        v = pickle.load(f)\n","    return v"],"metadata":{"id":"nWwThyviFU_H"},"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"xgc27_XoCfXh"},"cell_type":"code","source":["class Dataset(object):\n","  ''' Create a Class to store the data input and its features'''\n","  def __init__(self, filename: str, tokenize: Callable=simple_tokenizer, max_src_len: int=None,\n","               max_tgt_len: int=None, max_rows: int=None, truncate_src: bool=False, truncate_tgt: bool=False):\n","    print(\"Reading dataset %s...\" % filename, end=' ', flush=True)\n","    # Save the filename and initialize the variables\n","    self.filename = filename\n","    self.pairs = []\n","    self.src_len = 0\n","    self.tgt_len = 0\n","    self.max_rows = max_rows\n","\n","    #Read the csv file, using max rows if it is defined\n","    if max_rows is None:\n","        df = pd.read_csv(filename, encoding='utf-8')\n","    else:\n","        df = pd.read_csv(filename, encoding='utf-8', nrows=max_rows\n","                        )\n","    # Tokenize the source texts\n","    sources = df['finding'].apply(lambda x : tokenize(x))\n","    # Truncate the sources texts\n","    if truncate_src:\n","        sources = [src[:max_src_len] if len(src)>max_src_len else src for src in sources]\n","    # Tokenize the targets\n","    targets = df['impression'].apply(lambda x : tokenize(x))\n","    # Trucate the targets\n","    if truncate_tgt:\n","        targets = [tgt[:max_tgt_len] if len(tgt)>max_tgt_len else tgt for tgt in targets]\n","\n","    # Calculate the length of every source and targets\n","    src_length = [len(src)+1 for src in sources]\n","    tgt_length = [len(tgt)+1 for tgt in targets]\n","    #Calculate the max length of the sources and the targets\n","    max_src = max(src_length)\n","    max_tgt = max(tgt_length)\n","    #Create a tuple contaiing source,target,source length, target length\n","    self.src_len = max_src\n","    self.tgt_len = max_tgt\n","    # Insert the source text and target in the pairs class atribute\n","    self.pairs.append([(src, tgt, src_len, tgt_len) for src,tgt,src_len,tgt_len in zip(sources,targets,src_length,tgt_length)])\n","    self.pairs = self.pairs[0]\n","    print(\"%d pairs.\" % len(self.pairs))\n","\n","  def build_vocab(self, min_freq, embed_file: str=None) -> Vocab:\n","    ''' Build the vocabulary extracted from the texts in the object class\n","        Input:\n","        - min_freq: integer, minimum ocurrencies needed to include the word in the vocab\n","        - embed_file: string, path + filename of the embeddings file\n","    '''\n","    # Extract the words in the whole corpus\n","    total_words=[src+tgr for src,tgr,len_src,len_tgr in self.pairs]\n","    total_words = [item for sublist in total_words for item in sublist]\n","    # Create a counter to count the ocurrences of every word in the corpus\n","    word_counts = Counter(total_words)\n","    # Create a vocabulary object\n","    vocab=Vocab()\n","    for word,count in word_counts.items():\n","        # If occurences of the word are bigger then min_freq\n","        if(count>min_freq):\n","            # Include the word in the vocabulary\n","            vocab.add_words([word])\n","    # Load the embeddings in the vocab object\n","    count = vocab.load_embeddings(embed_file)\n","    print(\"%d pre-trained embeddings loaded.\" % count)\n","\n","    return vocab\n"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"G4D4m-5bCfXl"},"cell_type":"code","source":["class MyDataset(nn.Module):\n","    ''' A Dataset Class where we store all the data needed during the training phase'''\n","\n","    def __init__(self, src_sents, trg_sents, vocab):\n","      '''Initialize the instance and store the source texts, targets or summaries '''\n","      self.src_sents = src_sents\n","      self.trg_sents = trg_sents\n","      self.vocab=vocab\n","      # Keep track of how many data points.\n","      self._len = len(src_sents)\n","\n","    def __getitem__(self, index):\n","        ''' Return the ith items from the object\n","            Input:\n","            - Index: integer, index of the items to return\n","            Output:\n","            - a dictionary with keys x the source texts, y the targets,\n","              x_len length of source texts, y_len the length of targets\n","        '''\n","        return {'x':self.src_sents[index],\n","                'y':self.trg_sents[index],\n","                'x_len':len(self.src_sents[index]),\n","                'y_len':len(self.trg_sents[index])}\n","\n","    def __len__(self):\n","        ''' Return the length of the object'''\n","        return self._len\n"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"sOt9wBZJCfXn"},"cell_type":"code","source":["def tensorize(vocab, tokens):\n","    ''' Convert the tokens received to a tensor '''\n","    return torch.tensor([vocab[token] for token in tokens])\n","\n","def pad_sequence(vectorized_sent, max_len):\n","    ''' Padding the sentence (tensor) to max_len '''\n","    pad_dim = (0, max_len - len(vectorized_sent))\n","    return F.pad(vectorized_sent, pad_dim, 'constant').tolist()\n","\n","def preprocess(x,y,p,vocab):\n","    ''' Prepare a source text x and a target summary y: convert them to tensors,\n","        pads the sentences to its max length.\n","    '''\n","    # Convert x and y to tensors using the vocabulary\n","    tensors_src = tensorize(vocab, x)\n","    tensors_trg = tensorize(vocab, y)\n","    # Return the padded sequence of x and y and its length\n","    return {'x':pad_sequence(tensors_src, p.max_src_len),\n","          'y':pad_sequence(tensors_trg, p.max_tgt_len),\n","          'x_len':len(tensors_src),\n","          'y_len':len(tensors_trg)}\n","\n","def sort_batch_by_len(data_dict,p,vocab):\n","    ''' Return a batch of sentences processed and ordered by its length\n","    '''\n","    data=[]\n","    res={'x':[],'y':[],'x_len':[],'y_len':[]}\n","    # For every x and y in the data input\n","    for i in range(data_dict['x_len']):\n","        # Preprocess and tokenize the x and y\n","        data.append(preprocess(data_dict['x'][i],data_dict['y'][i],p,vocab))\n","    # For every preprocessed text, recreate the x and y lists\n","    for i in range(len(data)):\n","        res['x'].append(data[i]['x'])\n","        res['y'].append(data[i]['y'])\n","        res['x_len'].append(len(data[i]['x']))\n","        res['y_len'].append(len(data[i]['y']))\n","\n","    # Sort indices of data in batch by lengths\n","    sorted_indices = np.array(res['x_len']).argsort()[::-1].tolist()\n","    # Create a batch of data ordered by its length\n","    data_batch = {name:[_tensor[i] for i in sorted_indices]\n","                  for name, _tensor in res.items()}\n","    return data_batch\n"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"vswkUa11CfXp"},"cell_type":"code","source":["class EncoderRNN(nn.Module):\n","  ''' Define an encoder in a seq2seq architecture'''\n","  def __init__(self, embed_size, hidden_size, bidi=True, rnn_drop: float=0):\n","    super(EncoderRNN, self).__init__()\n","    # Set the hidden size\n","    self.hidden_size = hidden_size\n","    # Activate bidirectional mode\n","    self.num_directions = 2 if bidi else 1\n","    # Define the GRU layer of the encoder\n","    self.gru = nn.GRU(embed_size, hidden_size, bidirectional=bidi, dropout=rnn_drop)\n","\n","  def forward(self, embedded,hidden,input_lengths=None):\n","    ''' Run a Forward pass of the encoder to return outputs\n","        Input:\n","        - embedded: tensor, the embedding of the input data (word of the soure text)\n","        - hidden: a tensor, the previous hidden state of the encoder\n","        - input:lengths: a list of integers, length of the inputs\n","    '''\n","    # Pack the padded sequence of the embedded input\n","    if input_lengths is not None:\n","      input_lengths = input_lengths.cpu().type(torch.int64)\n","      embedded = pack_padded_sequence(embedded, input_lengths,batch_first=True)\n","\n","    # Apply the GRU layer of the encoder\n","    output, hidden = self.gru(embedded,hidden)\n","\n","    # Pad the sequence output\n","    if input_lengths is not None:\n","      output, _ = pad_packed_sequence(output)\n","    # If bidirectional\n","    if self.num_directions > 1:\n","      # Transform the hidden state tensor\n","      batch_size = hidden.size(1)\n","      hidden = hidden.transpose(0, 1).contiguous().view(1, batch_size,\n","                                                        self.hidden_size * self.num_directions)\n","    return output, hidden\n","\n","  def init_hidden(self, batch_size, device):\n","    ''' Initialize the hidden state of the encoder to zeros: num_directions, batch size, hidden size '''\n","    return torch.zeros(self.num_directions, batch_size, self.hidden_size, device=device)\n"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"TKg8ZR5TCfXq"},"cell_type":"code","source":["class DecoderRNN(nn.Module):\n","  ''' Define a decoder with atention in a seq2seq architecture'''\n","  def __init__(self, vocab_size, embed_size, hidden_size, enc_attn=True, dec_attn=True,\n","               enc_attn_cover=True, pointer=True,\n","               in_drop: float=0, rnn_drop: float=0, out_drop: float=0, enc_hidden_size=None,\n","               epsilon: float=0.0, device: str=\"cpu\"):\n","    ''' Initialize the decoder instance defining its parameters:\n","            Input:\n","                - vocab_size: integer, number of words in the vocabulary\n","                - embed_size: integer, size of the embedding layer\n","                - hidden_size: integer, size of the hidden layer (Hyperparameter)\n","                - enc_attn: activate the attention in the encoder\n","                - dec_attn: activate the attention in the decoder\n","                - enc_attn_cover: activate the coverage mechanism in the attention\n","                - pointer: activate the pointer generation\n","                - in_drop: dropout probability to apply to the input of the decoder\n","                - rnn_drop: dropout probability to apply to the GRU layer of the decoder\n","                - out_drop: dropout probability to apply to the output of the decoder\n","                - enc_hidden_size: dimension if the hidden state of the encoder\n","                - epsilon: float\n","                - device: cpu or gpu, device to store the tensors\n","    '''\n","\n","    super(DecoderRNN, self).__init__()\n","    # Set the attibutes\n","    self.vocab_size = vocab_size\n","    self.hidden_size = hidden_size\n","    self.combined_size = hidden_size\n","    self.device = device\n","    self.eps = epsilon\n","    # Define the input dropout layer\n","    self.in_drop = nn.Dropout(in_drop) if in_drop > 0 else None\n","    # Define the GRU layer\n","    self.gru = nn.GRU(embed_size, hidden_size, dropout=rnn_drop)\n","\n","    # Set the hidden size of the encoder to the hidden size of the decoder if it is not defined\n","    if not enc_hidden_size: enc_hidden_size = self.hidden_size\n","    # Bilinear layer of the encoder\n","    self.enc_bilinear = nn.Bilinear(hidden_size, enc_hidden_size, 1)\n","\n","    self.combined_size += enc_hidden_size\n","    if enc_attn_cover:\n","      # Initialize the weights of the coverage mechanism\n","      self.cover_weight = nn.Parameter(torch.rand(1))\n","\n","    # Bilinear layer of the encoder\n","    self.dec_bilinear = nn.Bilinear(self.hidden_size, self.hidden_size, 1)\n","    self.combined_size += self.hidden_size\n","\n","    # Define the output dropout layer\n","    self.out_drop = nn.Dropout(out_drop) if out_drop > 0 else None\n","    # Define the pointer generator layer\n","    self.ptr = nn.Linear(self.combined_size, 1)\n","\n","    # Define the linear layer at the output\n","    self.out = nn.Linear(self.combined_size, vocab_size)\n","\n","  def forward(self, embedded, hidden, encoder_hidden=None, decoder_states=None, coverage_vector=None, *,\n","              encoder_word_idx=None, ext_vocab_size: int=None, log_prob: bool=True):\n","    \"\"\"Run a Forward pass of the decoder to return outputs\n","        Input:\n","        - embedded: tensor, the embedding of the input data (decoder output in the last step\n","        - hidden: a tensor, the previous hidden state of the decoder\n","        - decoder_states: tensor, hidden state of the decoder in the last step\n","        - coverage_vector: tensor, coverage vector at this step\n","        - encoder_word_idx: tensor, indexes of the words in the source text\n","        - ext_vocab_size: integer, vocabulary size of the extended vocabulary\n","        - log_prob: bool, use of Log Softmax or Softmax in the output\n","    \"\"\"\n","    # Set the batch size and initialize the combined context vectors\n","    batch_size = embedded.size(0)\n","    combined = torch.zeros(batch_size, self.combined_size, device=self.device)\n","    # Apply the dropout layer to the input data\n","    if self.in_drop: embedded = self.in_drop(embedded)\n","    # Apply the GRU layer\n","    output, hidden = self.gru(embedded.unsqueeze(0), hidden)\n","    combined[:, :self.hidden_size] = output.squeeze(0)\n","    offset = self.hidden_size\n","    enc_attn, prob_ptr = None, None  # for visualization\n","\n","    # Encoder attention\n","    if encoder_hidden is not None:\n","        num_enc_steps = encoder_hidden.size(0)\n","        enc_total_size = encoder_hidden.size(2)\n","        # Apply the Bilinear layer\n","        enc_attn = self.enc_bilinear(hidden.expand(num_enc_steps, batch_size, -1).contiguous(), encoder_hidden)\n","\n","        # Update the attention weights\n","        if coverage_vector is not None:\n","            enc_attn += self.cover_weight * torch.log(coverage_vector.transpose(0, 1).unsqueeze(2) + self.eps)\n","        enc_attn = F.softmax(enc_attn, dim=0).transpose(0, 1)\n","\n","        # Calculate the context vectors\n","        enc_context = torch.bmm(encoder_hidden.permute(1, 2, 0), enc_attn)\n","        # Update the combined vector with the context vectors\n","        combined[:, offset:offset + enc_total_size] = enc_context.squeeze(2)\n","        offset += enc_total_size\n","        enc_attn = enc_attn.squeeze(2)\n","\n","    # Decoder attention\n","    if decoder_states is not None and len(decoder_states) > 0:\n","        dec_attn = self.dec_bilinear(hidden.expand_as(decoder_states).contiguous(), decoder_states)\n","        dec_attn = F.softmax(dec_attn, dim=0).transpose(0, 1)\n","        dec_context = torch.bmm(decoder_states.permute(1, 2, 0), dec_attn)\n","        combined[:, offset:offset + self.hidden_size] = dec_context.squeeze(2)\n","        offset += self.hidden_size\n","\n","    # Prepare the data to apply the pointer generator\n","    out_embed = combined\n","    logits = self.out(out_embed)\n","\n","    # Distribute probabilities between generator and pointer\n","    prob_ptr = torch.sigmoid(self.ptr(combined))\n","    prob_gen = 1 - prob_ptr\n","    # add generator probabilities to output\n","    gen_output = F.softmax(logits, dim=1)\n","    output = prob_gen * gen_output\n","    # Ensure output tensor has the correct size for scatter_add_\n","    if ext_vocab_size and output.size(1) < ext_vocab_size:\n","        pad_dim = (0, ext_vocab_size - output.size(1))\n","        output = F.pad(output, pad_dim, 'constant', 0)  # Pad with zeros\n","\n","    # add pointer probabilities to output\n","    ptr_output = enc_attn\n","    encoder_word_idx_l = encoder_word_idx.long()\n","\n","    # Clamp encoder_word_idx_l to be within the bounds of the output tensor\n","    encoder_word_idx_l = torch.clamp(encoder_word_idx_l, 0, output.size(1) - 1)\n","\n","    # Ensure tensors are contiguous and have correct shapes\n","    output = output.contiguous()\n","    scatter_src = (prob_ptr * ptr_output).contiguous()\n","\n","    # Check and adjust shape for scatter_add_ if necessary\n","    if scatter_src.dim() != encoder_word_idx_l.dim():\n","        scatter_src = scatter_src.unsqueeze(1).expand(-1, encoder_word_idx_l.size(1), -1)\n","        scatter_src = scatter_src.reshape(scatter_src.size(0), -1)\n","\n","    # Reshape index tensor if necessary\n","    if encoder_word_idx_l.dim() == 3:\n","        encoder_word_idx_l = encoder_word_idx_l.reshape(encoder_word_idx_l.size(0), -1)\n","\n","    # Calculate the output tensor\n","    output.scatter_add_(1, encoder_word_idx_l, scatter_src)\n","\n","    # Apply the log in the output\n","    output = torch.log(output + self.eps)\n","\n","    return output, hidden, enc_attn, prob_ptr"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"jrgAxxiqCfXs"},"cell_type":"code","source":["def get_coverage_vector(enc_attn_weights):\n","    \"\"\"Combine the past attention weights into one vector\"\"\"\n","    coverage_vector = torch.sum(torch.cat(enc_attn_weights), dim=0)\n","\n","    return coverage_vector\n"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"LvNIKvAbCfXt"},"cell_type":"code","source":["def get_next_batch(data, p, vocab, i, batch_size, device):\n","    ''' Generate and return the next batch of the data during training\n","        Input:\n","        - data: list, input data to the model\n","        - p: a class Parameters object, model and training parameters\n","        - vocab: a class Vocab object, vocabulary of the data\n","        - i: integer, index or iterator\n","        - batch_size: integer, batch size\n","        - device: string, where to train the model, cpu or gpu\n","    '''\n","    #Create a copy of the vocabulary\n","    vocab_ext=deepcopy(vocab)\n","\n","    #Get the next batch\n","    try:\n","        data_dict=data[i:i+batch_size]\n","    except:\n","        data_dict=data[i:len(data)]\n","    # Create a batch from an extended vocabulary\n","    data_batch = sort_batch_by_len(data_dict,p,vocab_ext)\n","    # Create an extended vocabulary\n","    for word in data_dict['x']:\n","        vocab_ext.add_words(word)\n","\n","    # Create a batch from an extended vocabulary\n","    data_batch_extra=sort_batch_by_len(data_dict,p,vocab_ext)\n","    #Create tha inputs in the extended version\n","    x_extra=torch.tensor(data_batch_extra['x']).to(device)\n","\n","    # Transform the batch to tensors\n","    x, x_len = torch.tensor(data_batch['x']).to(device), torch.tensor(data_batch['x_len']).to(device)\n","    y, y_len = torch.tensor(data_batch['y']).to(device), torch.tensor(data_batch['y_len']).to(device)\n","\n","    return x, x_len, y, y_len, x_extra, vocab_ext\n"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"E9WuuKG0CfXu"},"cell_type":"code","source":["def train(dataset,val_dataset,vocab,p,embedding_weights, learning_rate, num_epochs):\n","    ''' Run all the steps in the training phase\n","        Input:\n","        - dataset: Dataset object, training data\n","        - val_dataset: Dataset object, validation data\n","        - vocab: a class Vocab object, the vocabulary of the datasets\n","        - p: a class Parameters object, model and training parameters\n","        - embedding_weigths: tensor, the embedding vectors\n","        - learning_rate: float, learning rate parameter\n","        - num_epochs: integer, number of epochs of the training\n","    '''\n","    # Set some variables like eps, batch size and device\n","    eps = p.eps\n","    batch_size =p.batch_size\n","    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    #Create an adapter between encoder hidden state to decoder hidden size\n","    enc_dec_adapter = nn.Linear(p.hidden_size * 2, p.dec_hidden_size).to(DEVICE)\n","    #Create an embedding layer with pretrained weigths\n","    embedding = nn.Embedding(len(vocab), p.embed_size, padding_idx=vocab.PAD,\n","                             _weight=embedding_weights).to(DEVICE)\n","\n","    # Do not train the embeddings\n","    embedding.weight.requires_grad=False\n","    #Create the encoder\n","    encoder = EncoderRNN(p.embed_size, p.hidden_size, p.enc_bidi,rnn_drop=p.enc_rnn_dropout).to(DEVICE)\n","    #Create the decoder\n","    decoder = DecoderRNN(len(vocab), p.embed_size, p.dec_hidden_size,\n","                                  enc_attn=p.enc_attn, dec_attn=p.dec_attn,\n","                                  pointer=p.pointer,\n","                                  in_drop=p.dec_in_dropout, rnn_drop=p.dec_rnn_dropout,\n","                                  out_drop=p.dec_out_dropout, enc_hidden_size=p.hidden_size * 2,\n","                                  device=DEVICE, epsilon=p.eps).to(DEVICE)\n","\n","    # If the model components have been training, we restore them from a previous save\n","    if(os.path.exists(p.encoder_weights_path) and p.resume_train):\n","        encoder.load_state_dict(torch.load(p.encoder_weights_path,map_location=torch.device(DEVICE)))\n","    if(os.path.exists(p.decoder_weights_path) and p.resume_train):\n","        decoder.load_state_dict(torch.load(p.decoder_weights_path,map_location=torch.device(DEVICE)))\n","    if(os.path.exists(p.encoder_decoder_adapter_weights_path) and p.resume_train):\n","        enc_dec_adapter.load_state_dict(torch.load(p.encoder_decoder_adapter_weights_path,map_location=torch.device(DEVICE)))\n","\n","    # Create a Dataset class containing the training data\n","    cnn_data=MyDataset([pair[0] for pair in dataset.pairs],[pair[1] for pair in dataset.pairs],vocab)\n","\n","    # Create a Dataset class containing the validation data\n","    val_data=MyDataset([pair[0] for pair in val_dataset.pairs],[pair[1] for pair in val_dataset.pairs],vocab)\n","\n","\n","    # Define the loss function\n","    criterion = nn.NLLLoss(ignore_index=vocab.PAD)\n","    # Define the optimizers for the encoder, decoder and the adapter\n","    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n","    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n","    adapter_optimizer=optim.Adam([{'params':enc_dec_adapter.parameters()}], lr=learning_rate)\n","    # Record the losses\n","    losses=[]\n","    val_losses=[]\n","    #Load the losses from previous trainings\n","    if(os.path.exists(p.losses_path) and p.resume_train):\n","      with open(p.losses_path,'rb') as f:\n","        val_losses=pickle.load(f)\n","\n","    #Run training for num_epochs\n","    for _e in range(num_epochs):\n","        i=0\n","        #Create a progress bar\n","        print('\\nEpoch: %d/%d' % (_e + 1, num_epochs))\n","        kbar = pkbar.Kbar(target=len(cnn_data), width=8)\n","        #for every batch in the training data\n","        while i<len(cnn_data):\n","\n","            # Reset the gradients for the forward phase\n","            encoder_optimizer.zero_grad()\n","            decoder_optimizer.zero_grad()\n","            adapter_optimizer.zero_grad()\n","\n","            # Extract the data for the next batch\n","            x, x_len, y, y_len, x_extra, vocab_ext = get_next_batch(cnn_data, p, vocab, i, batch_size, device=DEVICE)\n","\n","            # Apply the embedding layer in the encoder\n","            encoder_embedded = embedding(x)\n","            # Create the init hidden state of the encoder\n","            encoder_hidden=encoder.init_hidden(x.size(0), DEVICE)\n","            # Forward pass in the encoder\n","            encoder_outputs, encoder_hidden =encoder(encoder_embedded,encoder_hidden,x_len)\n","            #Create the init input to the encoder\n","            decoder_input = torch.tensor([vocab.SOS] * x.size(0), device=DEVICE)\n","            # Adapt the encoder hidden to the encoder hidden size\n","            decoder_hidden = enc_dec_adapter(encoder_hidden)\n","\n","            decoder_states = []\n","            enc_attn_weights = []\n","            loss=0\n","            # For every token in the target\n","            for di in range(y.size(1)):\n","                #Apply the embedding layer to the decoder input\n","                decoder_embedded = embedding(decoder_input)\n","                # If activation of encoder attention is on\n","                if enc_attn_weights:\n","                    coverage_vector = get_coverage_vector(enc_attn_weights)\n","                else:\n","                    coverage_vector = None\n","\n","                #Forward pass to the decoder\n","                decoder_output, decoder_hidden, dec_enc_attn, dec_prob_ptr = decoder(decoder_embedded, decoder_hidden, encoder_outputs,\n","                            torch.cat(decoder_states) if decoder_states else None, coverage_vector,\n","                            encoder_word_idx=x_extra,log_prob=True,ext_vocab_size=len(vocab_ext))\n","                #Move the tensors to the device\n","                decoder_output.to(DEVICE)\n","                decoder_hidden.to(DEVICE)\n","                dec_enc_attn.to(DEVICE)\n","                dec_prob_ptr.to(DEVICE)\n","\n","                #Save the decoder hidden state\n","                decoder_states.append(decoder_hidden)\n","                #Calculate the probability distribution of the decoder outputs\n","                prob_distribution = torch.exp(decoder_output)\n","                #Get the largest element\n","                _, top_idx = decoder_output.data.topk(1)\n","                # Set the current target word to our goal\n","                gold_standard = y[:,di]\n","                # Apply the loss function\n","                nll_loss= criterion(decoder_output, gold_standard)\n","                loss+=nll_loss\n","\n","                #Set the decoder input to the target word or token\n","                decoder_input = y[:,di]\n","                #Calculate the coverage loss\n","                if (coverage_vector is not None and criterion):\n","                    coverage_loss = torch.sum(torch.min(coverage_vector, dec_enc_attn)) / batch_size #* cover_loss\n","                    loss+=coverage_loss\n","\n","                #Store the attention weights\n","                enc_attn_weights.append(dec_enc_attn.unsqueeze(0))\n","\n","            #Apply the backward to get the loss\n","            loss.backward()\n","            # Clipping the weights in the encoder, decoder and the adapter\n","            clip_grad_norm_(encoder.parameters(), 1)\n","            clip_grad_norm_(decoder.parameters(), 1)\n","            clip_grad_norm_(enc_dec_adapter.parameters(), 1)\n","            # Update the parameters\n","            encoder_optimizer.step()\n","            decoder_optimizer.step()\n","            adapter_optimizer.step()\n","            #Print the progress bar\n","            if i%(p.print_every*batch_size)==0:\n","                kbar.update(i, values=[(\"loss\", loss.data.item())])\n","            # Get the next batch\n","            i+=batch_size\n","\n","        # Calculate the final loss on the training\n","        loss=loss.data.item()/x.size(0)\n","        kbar.add(1, values=[(\"loss\", loss)])\n","\n","        #Repeat the process on the validation dataset\n","        kbar2 = pkbar.Kbar(target=len(val_data), width=8)\n","\n","        # calculating validation loss\n","        val_loss=0\n","        i=0\n","        while(i<len(val_data)):\n","            # Reset the gradients for the forward phase\n","            encoder_optimizer.zero_grad()\n","            decoder_optimizer.zero_grad()\n","            adapter_optimizer.zero_grad()\n","            # Get the next batch of the validation data\n","            x, x_len, y, y_len, x_extra, vocab_ext = get_next_batch(val_data, p, vocab, i, batch_size, device=DEVICE)\n","            # Forward pass of the encoder\n","            encoder_embedded = embedding(x)\n","            encoder_hidden=encoder.init_hidden(x.size(0), device=DEVICE)\n","            encoder_outputs, encoder_hidden =encoder(encoder_embedded,encoder_hidden,x_len)\n","            decoder_input = torch.tensor([vocab.SOS] * x.size(0), device=DEVICE)\n","            decoder_hidden = enc_dec_adapter(encoder_hidden)\n","\n","            decoder_states = []\n","            enc_attn_weights = []\n","            # For every word in the output\n","            for di in range(y.size(1)):\n","                try:\n","                    #Get the embedding vector of the input to the decoder\n","                    decoder_embedded = embedding(decoder_input)\n","                except:\n","                    print('Dec input: ',decoder_input.shape,' x:', x.shape,' x_len:',x_len.shape, ' Vocab:',\n","                          vocab.embeddings.shape,' Vocab Ext:', vocab_ext.embeddings.shape)\n","                    # Handle the IndexError: If decoder_input contains indices out of range for the embedding layer,\n","                    # we replace those indices with the UNK token index\n","                    decoder_input = torch.where(decoder_input >= embedding.num_embeddings, torch.tensor(vocab.UNK, device=decoder_input.device), decoder_input)\n","                    decoder_embedded = embedding(decoder_input) # Re-apply embedding with corrected indices\n","                # Generate the coverage vectors if neccessary\n","                if enc_attn_weights:\n","                    coverage_vector = get_coverage_vector(enc_attn_weights)\n","                else:\n","                    coverage_vector = None\n","                # Fordward pass to the decoder\n","                decoder_output, decoder_hidden, dec_enc_attn, dec_prob_ptr = decoder(decoder_embedded, decoder_hidden, encoder_outputs,\n","                            torch.cat(decoder_states) if decoder_states else None, coverage_vector,\n","                            encoder_word_idx=x_extra,log_prob=True,ext_vocab_size=len(vocab_ext))\n","                # Move the tensors to the device, gpu or cpu\n","                decoder_output.to(DEVICE)\n","                decoder_hidden.to(DEVICE)\n","                dec_enc_attn.to(DEVICE)\n","                dec_prob_ptr.to(DEVICE)\n","                # Stores the hidden states of the decoder\n","                decoder_states.append(decoder_hidden)\n","                prob_distribution = torch.exp(decoder_output)\n","                # Get the output token with the highest probability\n","                _, top_idx = decoder_output.data.topk(1)\n","                gold_standard = y[:,di]\n","                # Apply the loss function\n","                nll_loss= criterion(decoder_output, gold_standard)\n","                val_loss+=nll_loss.data.item()\n","\n","                # Set the decoder input to the last output from the decoder\n","                decoder_input = top_idx.view(-1)\n","                # update the coverage vector\n","                if (coverage_vector is not None and criterion):\n","                    coverage_loss = torch.sum(torch.min(coverage_vector, dec_enc_attn)) / batch_size\n","                    val_loss+=coverage_loss.data.item()\n","                # Collect the attention weights in the step\n","                enc_attn_weights.append(dec_enc_attn.unsqueeze(0))\n","            # Print the progress\n","            if i%(p.print_every*batch_size)==0:\n","                kbar2.update(i, values=[(\"Val loss\", val_loss)])\n","\n","            i+=batch_size\n","\n","        #Calculate the validation loss\n","        avg_val_loss=val_loss/len(val_data)\n","        kbar2.add(1, values=[(\"Train loss\", loss), (\"Val loss\", val_loss), (\"Avg Val loss\", avg_val_loss)])\n","\n","        # Save the mnodel and results to disk\n","        if(len(val_losses)>0 and avg_val_loss<min(val_losses)):\n","            torch.save(encoder.state_dict(), p.encoder_weights_path)\n","            torch.save(decoder.state_dict(), p.decoder_weights_path)\n","            torch.save(enc_dec_adapter.state_dict(), p.encoder_decoder_adapter_weights_path)\n","        val_losses.append(avg_val_loss)\n","\n","    with open(p.losses_path,'wb') as f:\n","        pickle.dump(val_losses,f)\n"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"KWS5kl4YCfXw"},"cell_type":"code","source":["def predict(sent,vocab,p,batch_size=1):\n","    ''' Function to predict the summary of the source text sentence\n","        Input:\n","        - sent: string, text to summarize\n","        - vocab: a class Vocab object, vocabulary of the texts\n","        - p: a class Parameters object, model parameters\n","        - batch_size: integer, batch size of the data to predict\n","    '''\n","    eps=p.eps\n","    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    # Create a tensor with the embedding vectors\n","    embedding_weights = torch.from_numpy(vocab.embeddings).to(DEVICE)\n","    # Create the layer to transform inputs between encoder and decoder\n","    enc_dec_adapter = nn.Linear(p.hidden_size * 2, p.dec_hidden_size).to(DEVICE)\n","    # Create the embedding layer using the embedding vectors\n","    embedding = nn.Embedding(len(vocab), p.embed_size, padding_idx=vocab.PAD,_weight=embedding_weights).to(DEVICE)\n","    # Create the encoder and the decoder\n","    encoder = EncoderRNN(p.embed_size, p.hidden_size, p.enc_bidi,rnn_drop=p.enc_rnn_dropout).to(DEVICE)\n","    decoder = DecoderRNN(len(vocab), p.embed_size, p.dec_hidden_size,\n","                                  enc_attn=p.enc_attn, dec_attn=p.dec_attn,\n","                                  pointer=p.pointer,\n","                                  in_drop=p.dec_in_dropout, rnn_drop=p.dec_rnn_dropout,\n","                                  out_drop=p.dec_out_dropout, enc_hidden_size=p.hidden_size * 2,\n","                                  device=DEVICE).to(DEVICE)\n","    # Tokenize the input text\n","    sent_vec=[vocab[word] for word in sent.split()]\n","    vocab_ext=deepcopy(vocab)\n","    # Extend the vocabulary\n","    for word in sent.split():\n","      vocab_ext.add_words(word)\n","    # Tokenize the text with the extended  vocabulary\n","    sent_vec_extra=[vocab_ext[word] for word in sent.split()]\n","    # Pad the text if neccessary\n","    if(len(sent_vec_extra)<p.max_src_len):\n","        pad_dim = (0, p.max_src_len-len(sent_vec_extra))\n","        sent_vec_extra_tensor=F.pad(torch.tensor(sent_vec_extra), pad_dim , 'constant')\n","    else:\n","        sent_vec_extra_tensor=torch.tensor(sent_vec_extra)\n","\n","    # Pad the text if neccessary\n","    if(len(sent_vec)<p.max_src_len):\n","        pad_dim = (0, p.max_src_len-len(sent_vec))\n","        sent_vec_tensor=F.pad(torch.tensor(sent_vec), pad_dim, 'constant')\n","    else:\n","        sent_vec_tensor=torch.tensor(sent_vec)\n","\n","    # Load the encoder model from file\n","    if(os.path.exists(p.encoder_weights_path)):\n","        encoder.load_state_dict(torch.load(p.encoder_weights_path,map_location=torch.device(DEVICE)))\n","    # Load the decoder model from file\n","    if(os.path.exists(p.decoder_weights_path)):\n","        decoder.load_state_dict(torch.load(p.decoder_weights_path,map_location=torch.device(DEVICE)))\n","    # Load the encoder -decoder adapter component from file\n","    if(os.path.exists(p.encoder_decoder_adapter_weights_path)):\n","        enc_dec_adapter.load_state_dict(torch.load(p.encoder_decoder_adapter_weights_path,map_location=torch.device(DEVICE)))\n","\n","    x=sent_vec_tensor.view(1,-1).to(DEVICE)\n","    x_extra=sent_vec_extra_tensor.view(1,-1).to(DEVICE)\n","    # Apply the embedding layer\n","    encoder_embedded = embedding(x)\n","    # Initiaize the hidden state of the encoder\n","    encoder_hidden=encoder.init_hidden(x.size(0), DEVICE)\n","    # Fordward pass to the encoder\n","    encoder_outputs, encoder_hidden =encoder(encoder_embedded,encoder_hidden,\n","                                             torch.tensor(p.max_src_len).view(1).to(DEVICE))\n","    # Initialize the decoder input to SOS tokens\n","    decoder_input = torch.tensor([vocab.SOS] * batch_size, device=DEVICE)\n","    decoder_hidden = enc_dec_adapter(encoder_hidden)\n","\n","    #Initialize the hidden states and weights of some variables\n","    decoder_states = []\n","    enc_attn_weights = []\n","    output=[]\n","    # For every word in the output sequence\n","    for di in range(p.max_tgt_len):\n","        # Apply the embedding layer of the decoder\n","        decoder_embedded = embedding(decoder_input)\n","        # Get the coverage vector\n","        if enc_attn_weights:\n","            coverage_vector = get_coverage_vector(enc_attn_weights)\n","        else:\n","            coverage_vector = None\n","        # fordward pass to the decoder\n","        decoder_output, decoder_hidden, dec_enc_attn, dec_prob_ptr = decoder(decoder_embedded, decoder_hidden, encoder_outputs,\n","                    torch.cat(decoder_states).to(DEVICE) if decoder_states else None, coverage_vector,\n","                    encoder_word_idx=x_extra,log_prob=True,ext_vocab_size=len(vocab_ext))\n","        # Move the tensor to the device\n","        decoder_output.to(DEVICE)\n","        decoder_hidden.to(DEVICE)\n","        dec_enc_attn.to(DEVICE)\n","        dec_prob_ptr.to(DEVICE)\n","        # Store the hidden state of the decoder\n","        decoder_states.append(decoder_hidden)\n","        # Generate the probability distribution\n","        prob_distribution = torch.exp(decoder_output)\n","        # Get the element in the decoder output with the highest probability (the best output)\n","        _, top_idx = decoder_output.data.topk(1)\n","        # Store the output (word) to the output text\n","        output.append(top_idx.squeeze().data.item())\n","        # Store the encoder attention weights\n","        enc_attn_weights.append(dec_enc_attn.unsqueeze(0))\n","        # Set the decoder input in the next iter\n","        decoder_input = top_idx.view(-1)\n","    # Transform the outputs (words) to a list of text or words\n","    output=[vocab_ext[idx] for idx in output]\n","    return output"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"4XlLtLW6CfXx"},"cell_type":"code","source":["def prediction(sent,vocab,embedding, encoder, enc_dec_adapter, decoder, device, p,batch_size=1):\n","    ''' Function to predict the summary of the source text sentence\n","        Input:\n","        - sent: string, text to summarize\n","        - vocab: a class Vocab object, vocabulary of the texts\n","        - p: a class Parameters object, model parameters\n","        - batch_size: integer, batch size of the data to predict\n","    '''\n","    eps=p.eps\n","    # Tokenize the input text\n","    sent_vec=[vocab[word] for word in sent.split()]\n","    vocab_ext=deepcopy(vocab)\n","    # Extend the vocabulary\n","    for word in sent.split():\n","      vocab_ext.add_words(word)\n","    # Tokenize the text with the extended vocabulary\n","    sent_vec_extra=[vocab_ext[word] for word in sent.split()]\n","    # Pad the text if neccessary\n","    if(len(sent_vec_extra)<p.max_src_len):\n","        pad_dim = (0, p.max_src_len-len(sent_vec_extra))\n","        sent_vec_extra_tensor=F.pad(torch.tensor(sent_vec_extra), pad_dim , 'constant')\n","    else:\n","        sent_vec_extra_tensor=torch.tensor(sent_vec_extra)\n","\n","    # Pad the text if neccessary\n","    if(len(sent_vec)<p.max_src_len):\n","        pad_dim = (0, p.max_src_len-len(sent_vec))\n","        sent_vec_tensor=F.pad(torch.tensor(sent_vec), pad_dim, 'constant')\n","    else:\n","        sent_vec_tensor=torch.tensor(sent_vec)\n","\n","    x=sent_vec_tensor.view(1,-1).to(device)\n","    x_extra=sent_vec_extra_tensor.view(1,-1).to(device)\n","    # Apply the embedding layer\n","    encoder_embedded = embedding(x)\n","    # Initiaize the hidden state of the encoder\n","    encoder_hidden=encoder.init_hidden(x.size(0), device)\n","    # Fordward pass to the encoder\n","    encoder_outputs, encoder_hidden =encoder(encoder_embedded,encoder_hidden,\n","                                             torch.tensor(p.max_src_len).view(1).to(device))\n","    # Initialize the decoder input to SOS tokens\n","    decoder_input = torch.tensor([vocab.SOS] * batch_size, device=device)\n","    decoder_hidden = enc_dec_adapter(encoder_hidden)\n","\n","    #Initialize the hidden states and weights of some variables\n","    decoder_states = []\n","    enc_attn_weights = []\n","    output=[]\n","    # For every word in the output sequence\n","    for di in range(p.max_tgt_len):\n","        # Apply the embedding layer of the decoder\n","        decoder_embedded = embedding(decoder_input)\n","        # Add this line to handle the IndexError:\n","        decoder_input = torch.where(decoder_input >= embedding.num_embeddings, torch.tensor(vocab.UNK, device=decoder_input.device), decoder_input)\n","        decoder_embedded = embedding(decoder_input) # Re-apply embedding\n","        # Get the coverage vector\n","        if enc_attn_weights:\n","            coverage_vector = get_coverage_vector(enc_attn_weights)\n","        else:\n","            coverage_vector = None\n","\n","        # fordward pass to the decoder\n","        decoder_output, decoder_hidden, dec_enc_attn, dec_prob_ptr = decoder(decoder_embedded, decoder_hidden, encoder_outputs,\n","                    torch.cat(decoder_states).to(device) if decoder_states else None, coverage_vector,\n","                    encoder_word_idx=x_extra,log_prob=True,ext_vocab_size=len(vocab_ext))\n","        # Move the tensor to the device\n","        decoder_output.to(device)\n","        decoder_hidden.to(device)\n","        dec_enc_attn.to(device)\n","        dec_prob_ptr.to(device)\n","        # Store the hidden state of the decoder\n","        decoder_states.append(decoder_hidden)\n","        # Generate the probability distribution\n","        prob_distribution = torch.exp(decoder_output)\n","        # Get the element in the decoder output with the highest probability (the best output)\n","        _, top_idx = decoder_output.data.topk(1)\n","        # Store the output (word) to the output text\n","        output.append(top_idx.squeeze().data.item())\n","        # Store the encoder attention weights\n","        enc_attn_weights.append(dec_enc_attn.unsqueeze(0))\n","        # Set the decoder input in the next iter\n","        decoder_input = top_idx.view(-1)\n","\n","    # Transform the outputs (words) to a list of text or words\n","    output=[vocab_ext[idx] for idx in output]\n","    return output"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"QqttMlitCfXy"},"cell_type":"code","source":["def eval_metrics(preds, targets, avg=True):\n","    ''' Evaluate the ROUGE metrics ROUGE-2 and ROUGE-L for every pair predicted summary - target summary\n","\n","        Input:\n","           - preds: list of strings, predicted summaries\n","           - targets: list of string, target summaries\n","        Output:\n","            - rouge2_f_metric: list of float, the Rouge-2 fscore for every predicted summary\n","            - rougel_f_metric: list of float, the Rouge-L fscore for every predicted summary\n","    '''\n","    #Lets calculate the rouge metrics for every document\n","    rouge = Rouge()\n","    scores = rouge.get_scores(preds, targets, avg)\n","    # Create the output variables\n","    if avg:\n","        rouge2_f_metric = scores['rouge-2']['f']\n","        rouge2_p_metric = scores['rouge-2']['p']\n","        rouge2_r_metric = scores['rouge-2']['r']\n","        rougel_f_metric = scores['rouge-l']['f']\n","        rougel_p_metric = scores['rouge-l']['p']\n","        rougel_r_metric = scores['rouge-l']['r']\n","    else:\n","        rouge2_f_metric = [score['rouge-2']['f'] for score in scores]\n","        rouge2_p_metric = [score['rouge-2']['p'] for score in scores]\n","        rouge2_r_metric = [score['rouge-2']['r'] for score in scores]\n","        rougel_f_metric = [score['rouge-l']['f'] for score in scores]\n","        rougel_p_metric = [score['rouge-l']['p'] for score in scores]\n","        rougel_r_metric = [score['rouge-l']['r'] for score in scores]\n","\n","    return rouge2_f_metric, rouge2_p_metric, rouge2_r_metric, rougel_f_metric, rougel_p_metric, rougel_r_metric\n","\n","def save_to_df(text, labeled_summaries, predicted_summaries, r2_f, r2_p, r2_r, rl_f, rl_p, rl_r):\n","    ''' Stores the metric results into a pandas dataframe'''\n","    results = pd.DataFrame(columns=['text', 'summary','pred_summary','rouge2-f','rouge2-p','rouge2-r','rougel-f', 'rougel-p', 'rougel-r'])\n","    results['text'] = text\n","    results['summary'] = labeled_summaries\n","    results['pred_summary'] = predicted_summaries\n","    results['rouge2-f'] = r2_f\n","    results['rouge2-p'] = r2_p\n","    results['rouge2-r'] = r2_r\n","    results['rougel-f'] = rl_f\n","    results['rougel-p'] = rl_p\n","    results['rougel-r'] = rl_r\n","\n","    return results"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"zvrPogBiCfXy"},"cell_type":"code","source":["def get_predictions(x_test, vocab, params, print_every=20):\n","    ''' Generate the predicted summaries of the source texts on x_test\n","        Input:\n","        - x_test: list of strings, the source texts\n","        - vocab: a Vocab Class object, vocabulary of the texts\n","        - params: a Parameters object, parameter of the model\n","        - print_every: integer, print progress every print_every iterations\n","    '''\n","    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    # Create a tensor with the embedding vectors\n","    embedding_weights = torch.from_numpy(vocab.embeddings).to(DEVICE)\n","    # Create the layer to transform inputs between encoder and decoder\n","    enc_dec_adapter = nn.Linear(params.hidden_size * 2, params.dec_hidden_size).to(DEVICE)\n","    # Create the embedding layer using the embedding vectors\n","    embedding = nn.Embedding(len(vocab), params.embed_size, padding_idx=vocab.PAD,_weight=embedding_weights).to(DEVICE)\n","    # Create the encoder and the decoder\n","    encoder = EncoderRNN(params.embed_size,params.hidden_size, params.enc_bidi,rnn_drop=params.enc_rnn_dropout).to(DEVICE)\n","    decoder = DecoderRNN(len(vocab), params.embed_size, params.dec_hidden_size,\n","                                  enc_attn=params.enc_attn, dec_attn=params.dec_attn,\n","                                  pointer=params.pointer,\n","                                  in_drop=params.dec_in_dropout, rnn_drop=params.dec_rnn_dropout,\n","                                  out_drop=params.dec_out_dropout, enc_hidden_size=params.hidden_size * 2,\n","                                  device=DEVICE).to(DEVICE)\n","\n","    # Load the encoder model from file\n","    if(os.path.exists(params.encoder_weights_path)):\n","        encoder.load_state_dict(torch.load(params.encoder_weights_path,map_location=torch.device(DEVICE)))\n","    # Load the decoder model from file\n","    if(os.path.exists(params.decoder_weights_path)):\n","        decoder.load_state_dict(torch.load(params.decoder_weights_path,map_location=torch.device(DEVICE)))\n","    # Load the encoder-decoder adapter component from file\n","    if(os.path.exists(params.encoder_decoder_adapter_weights_path)):\n","        enc_dec_adapter.load_state_dict(torch.load(params.encoder_decoder_adapter_weights_path,map_location=torch.device(DEVICE)))\n","\n","\n","    predicted_summaries = []\n","    # Set a progress bar\n","    kbar = pkbar.Kbar(target=len(x_test), width=8)\n","    # For every text in the validation dataset\n","    for i,doc in enumerate(x_test):\n","        # Predict the summary for the document\n","        pred_summ = prediction(doc,vocab,embedding,encoder,enc_dec_adapter,decoder,DEVICE,params,batch_size=1)\n","        predicted_summaries.append(' '.join(pred_summ))\n","        #Show the progress\n","        if i%print_every==0:\n","            kbar.update(i)\n","\n","    # Set the labeled summaries as the y_test variable, column summary of our dataset\n","    return predicted_summaries\n"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"8dRyjPekCfXz"},"cell_type":"code","source":["def generate_predictions(x_test, vocab, params, print_every=20):\n","    ''' Generate the predicted summaries of the source texts on x_test\n","        Input:\n","        - x_test: list of strings, the source texts\n","        - vocab: a Vocab Class object, vocabulary of the texts\n","        - params: a Parameters object, parameter of the model\n","        - print_every: integer, print progress every print_every iterations\n","    '''\n","\n","    predicted_summaries = []\n","    # Set a progress bar\n","    kbar = pkbar.Kbar(target=len(x_test), width=8)\n","    # For every text in the validation dataset\n","    for i,doc in enumerate(x_test):\n","        # Predict the summary for the document\n","        pred_summ = predict(doc,vocab,params,batch_size=1)\n","        predicted_summaries.append(' '.join(pred_summ))\n","        #Show the progress\n","        if i%print_every==0:\n","            kbar.update(i)\n","\n","    # Set the labeled summaries as the y_test variable, column summary of our dataset\n","    return predicted_summaries\n"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"HXNESvQxCfX0"},"cell_type":"code","source":["# Create an object with the model and training parameters\n","params = Parameters()"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"9JYi63MjCfX0","executionInfo":{"status":"ok","timestamp":1744808552951,"user_tz":-330,"elapsed":60,"user":{"displayName":"Anindita Bhattacharya","userId":"16794923920170422248"}},"outputId":"c6d8ae64-df78-4b99-e384-cbe894bcf0a8"},"cell_type":"code","source":["# Load the training dataset using the simple tokenizer\n","dataset = Dataset(params.train_data_path, simple_tokenizer, params.max_src_len, params.max_tgt_len, max_rows=64000,\n","                        truncate_src=True, truncate_tgt=True)\n","# Load the validation dataset using the simple tokenizer\n","valid_dataset = Dataset(params.val_data_path, simple_tokenizer, params.max_src_len, params.max_tgt_len, max_rows= 3200,\n","                        truncate_src=True, truncate_tgt=True)\n","#Show the length to check the loadings\n","print(dataset.src_len, valid_dataset.src_len,dataset.tgt_len, valid_dataset.tgt_len)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading dataset /content/drive/MyDrive/train_split.csv... 2400 pairs.\n","Reading dataset /content/drive/MyDrive/val_split.csv... 600 pairs.\n","66 66 16 16\n"]}]},{"metadata":{"trusted":true,"id":"npz8QXI3CfX1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1744808557939,"user_tz":-330,"elapsed":3904,"user":{"displayName":"Anindita Bhattacharya","userId":"16794923920170422248"}},"outputId":"eec29e28-8f1c-4478-d875-d01b3cf11806"},"cell_type":"code","source":["# convert the embeddings to a tensor\n","vocab = dataset.build_vocab(params.vocab_min_frequency, embed_file=params.embed_file)\n","vocab.save_to_file('vocab_train.pkl')\n","# convert the embeddings to a tensor\n","embedding_weights = torch.from_numpy(vocab.embeddings)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["945 pre-trained embeddings loaded.\n"]}]},{"cell_type":"code","source":["TORCH_USE_CUDA_DSA = True"],"metadata":{"id":"3aGT-gw8hZ_-"},"execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"QSki7z_LCfX9","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ab347455-574c-4433-fa06-e93508108a6c","executionInfo":{"status":"ok","timestamp":1744816343917,"user_tz":-330,"elapsed":2087496,"user":{"displayName":"Anindita Bhattacharya","userId":"16794923920170422248"}}},"cell_type":"code","source":["train(dataset,valid_dataset,vocab, params, embedding_weights,learning_rate=3e-5,num_epochs = 5)"],"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch: 1/5\n","  0/600 [........] - ETA: 0s - Val loss: 0.0000e+00Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","  1/600 [........] - ETA: 7:44:16 - Val loss: 3993.6751 - Train loss: 6.6878 - Avg Val loss: 6.6561\n","Epoch: 2/5\n","  0/600 [........] - ETA: 0s - Val loss: 0.0000e+00Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","  1/600 [........] - ETA: 8:24:08 - Val loss: 3724.2276 - Train loss: 6.0343 - Avg Val loss: 6.2070\n","Epoch: 3/5\n","1601/2400 [====>...] - ETA: 12:21 - loss: 91.0582Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","  0/600 [........] - ETA: 0s - Val loss: 0.0000e+00Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","  1/600 [........] - ETA: 8:37:08 - Val loss: 3437.0799 - Train loss: 5.2901 - Avg Val loss: 5.7285\n","Epoch: 4/5\n","1601/2400 [====>...] - ETA: 12:28 - loss: 87.5792Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","  0/600 [........] - ETA: 0s - Val loss: 0.0000e+00Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","  1/600 [........] - ETA: 8:17:13 - Val loss: 3381.7830 - Train loss: 5.1365 - Avg Val loss: 5.6363\n","Epoch: 5/5\n","1601/2400 [====>...] - ETA: 12:36 - loss: 85.6329Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","  0/600 [........] - ETA: 0s - Val loss: 0.0000e+00Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","Dec input:  torch.Size([16])  x: torch.Size([16, 65])  x_len: torch.Size([16])  Vocab: (1699, 100)  Vocab Ext: (1699, 100)\n","  1/600 [........] - ETA: 8:21:11 - Val loss: 3363.1639 - Train loss: 5.0400 - Avg Val loss: 5.6053"]}]},{"metadata":{"trusted":true,"id":"RaM93C33CfX-","executionInfo":{"status":"ok","timestamp":1744817070940,"user_tz":-330,"elapsed":715353,"user":{"displayName":"Anindita Bhattacharya","userId":"16794923920170422248"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"36128186-b399-40f5-b8c2-ff1ed8456490"},"cell_type":"code","source":["test_dataset = Dataset(params.test_data_path, simple_tokenizer, params.max_src_len, params.max_tgt_len, max_rows= 3200,\n","                        truncate_src=True, truncate_tgt=True)\n","# Prepare the validation dataset to be used in the evaluation\n","print('Length Test Dataset:', len(test_dataset.pairs))\n","x_test = [' '.join(pair[0]) for pair in test_dataset.pairs]\n","y_test = [' '.join(pair[1]) for pair in test_dataset.pairs]\n","# Predict the summaries\n","#preds = generate_predictions(x_test, vocab, params, print_every=10)\n","preds = get_predictions(x_test, vocab, params, print_every=100)\n","# Calculate the Rouge-2 and Rouge-L metrics for the validation dataset\n","r2_f, r2_p, r2_r, rl_f, rl_p, rl_r = eval_metrics(preds, y_test, False)\n","print('\\nMean Rouge-2 FScore: ',np.mean(r2_f), 'Mean Rouge-L FScore: ',np.mean(rl_f))\n","# Store the evaluation results to a CSV file\n","test_results = save_to_df(x_test, y_test, preds, r2_f, r2_p, r2_r, rl_f, rl_p, rl_r)\n","test_results.to_csv('test_results_pointer_gen.csv', index=False)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading dataset /content/drive/MyDrive/test_split.csv... 500 pairs.\n","Length Test Dataset: 500\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["400/500 [=====>..] - ETA: 2:23\n","Mean Rouge-2 FScore:  0.051072068370370385 Mean Rouge-L FScore:  0.1731227899089281\n"]}]},{"cell_type":"code","source":["# Convert to CSV\n","file_path = \"/content/drive/MyDrive/ME THESIS/Predicted Summaries/PGNwithCOV.csv\"\n","# Use the to_csv method to write to the file\n","test_results.to_csv(file_path, index=False)"],"metadata":{"id":"w1zA0y1PkHw2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the CSV file containing the model generated summaries\n","test_results = pd.read_csv(\"/content/drive/MyDrive/ME THESIS/Predicted Summaries/PGNwithCOV.csv\")"],"metadata":{"id":"7FmeHB-y-D5o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Extract ground truth (impression) and generated summaries\n","references = test_results[\"summary\"].astype(str).tolist()\n","predictions = test_results[\"pred_summary\"].astype(str).tolist()"],"metadata":{"id":"jelcDp9I4xCZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialize ROUGE scorer\n","rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)"],"metadata":{"id":"r4rUion4y5sO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Compute ROUGE, METEOR, and store them\n","rouge1_scores, rouge2_scores, rougeL_scores = [], [], []"],"metadata":{"id":"RV4w2cm0zN2P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from nltk.translate.meteor_score import meteor_score"],"metadata":{"id":"mD0MX_VXzNzc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import nltk\n","nltk.download('wordnet')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LDjJNroJzNwz","executionInfo":{"status":"ok","timestamp":1745737713936,"user_tz":-330,"elapsed":145,"user":{"displayName":"Anindita Bhattacharya","userId":"16794923920170422248"}},"outputId":"62146bfe-4e27-4e07-8b0e-b29165e3022f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["for ref, pred in zip(references, predictions):\n","    # Compute ROUGE scores\n","    scores = rouge.score(ref, pred)\n","    rouge1_scores.append(scores['rouge1'].fmeasure)\n","    rouge2_scores.append(scores['rouge2'].fmeasure)\n","    rougeL_scores.append(scores['rougeL'].fmeasure)"],"metadata":{"id":"p9RSX55izNuq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from bert_score import score"],"metadata":{"id":"uJuGGXoRzNoz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Compute BERTScore\n","P, R, F1 = score(predictions, references, lang=\"en\", verbose=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":298,"referenced_widgets":["d8039c9299744559b3f64ba81feb8bf9","3fc7fa4c1ddd4a91bcc9019af02f40e4","c6621760e2f24916a58c8541395292dc","c1e4bbecd8c643f090e221f0c4723c9a","c3983c518d3b4d7ead89f5bd7a12d106","872114ab5e8141cfaf9017cacb3ca3e0","c1ee8100cad447b49593c7901d15cfd0","66d39d88ee20442c8f6f81768526b912","ba87e0c2fa2540afa6208328db19e0f9","41be995f4b64452cb706fecdb72082d0","2ec2b8733eaa486c83e96a0b0b15a29d","31e3a7ec8d4843c9ba0bc86298e2a67f","41f31fd356fe41a19893a94cea5b8da6","c051656f7a2d4f818f60634a073b3c1d","ee15f4198a4c4a68a2610e3f9faba437","ae09b682875742fdb4f7de86a5856900","d62eab05c8434ffebfd9dbba9ed85052","3c78b37bcc324a4c8af17bcebff9dfb5","31c37c2ac3a9429ea808c7ae2d142dfb","903b2f0c21b547af9a5d870f876379bc","142b18d7504346208438dbc26c7840d1","a6c2360d642a4edbbc4e72c2a8b832d4","2d201e38fbd54672b7cf789216ae2f55","ac29c0ed146d456880bb442281f19b00","224e08ec7b684e8d88bc437f44dc55ce","acf250e0f5414e4bbd05d405f1f682b4","dd70ff0ae2544637b01c3fe340ae9518","4948e962cd0440bdb330be424871f357","7774c95fe0eb434faf43ab79542c521c","8bd66190bee44bc6b19e2241c1cdfd06","0630619c850442b89bf7ae3b193df57d","efb0a722612c4f04870801cf9b0ddde4","24963ef522db4285b15c984f70bddb36","30bc0e516bfa4058ac8297972115ff7d","0bdb48c93f4f4d43a1066d2ee681cead","dd626b2e8f134c9eb875391ae1cc6660","3b80cfed981b418d801d0d7cbab622f1","c335f31aec3b42e99a7bc65836eb21cb","529c07ba615e4d29a0a3421411861159","843f393dff0743498087c328f2abdf9e","574ee72979e54c3a995bd2071bbc1efe","2c74646a34f14b20beb66e52cd0b5b9e","506882a3055344b9900d6a0d2354ad6e","4161377037294890be40c654f2b14ea0","9496bec3764c46ac9257c272f1733a20","940dbe82a5a542a5973cbebe02a1b500","7edb61000ee14853b390f471190bc417","251fa0f9c75a41f997a7ecdbafbcb087","9d1f99fe17fb476c96b344f39a38b0fc","197443e0b87c47b09e917a281182562d","75612a3c74c84c2d89bab7695bcfe88f","5fd5dda304464697952e9891aaa94c55","b1a9420ce80140d29c99b6828d54556b","463d0523782a4b0d945b2343e6112125","3e0cd5d45d3e430db48265efd252cc51","4140730bcb0f479f96850c85e1d8ce5d","607bac5033794c70bbd3dcff015a8924","97de5c649b61436caf59a9ddfd7f16f1","ec4fd8a08a5f4e55908d159ad272d753","a625edfebe7245e891e539f4e08e39f0","81168e222bb5496c966c3e9d7708e873","a24428797ca541e4a86d68a8cdf12bce","2ffecd15eb1d45e5b4192bc73a1d72cf","69fa9475e4d34ea9becdd231304aa058","1830da3095ec435b9a44b460fce8dafd","aa2fa65fadf54425b293b06445c83247"]},"id":"1siSwKF8zNly","executionInfo":{"status":"ok","timestamp":1745737884347,"user_tz":-330,"elapsed":163077,"user":{"displayName":"Anindita Bhattacharya","userId":"16794923920170422248"}},"outputId":"c94f9636-0bde-48ea-ca7f-cd638b44dd9c"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8039c9299744559b3f64ba81feb8bf9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31e3a7ec8d4843c9ba0bc86298e2a67f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d201e38fbd54672b7cf789216ae2f55"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30bc0e516bfa4058ac8297972115ff7d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9496bec3764c46ac9257c272f1733a20"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"]},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4140730bcb0f479f96850c85e1d8ce5d"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["# Calculate averages\n","average_scores = {\n","    \"ROUGE-1\": sum(rouge1_scores) / len(rouge1_scores),\n","    \"ROUGE-2\": sum(rouge2_scores) / len(rouge2_scores),\n","    \"ROUGE-L\": sum(rougeL_scores) / len(rougeL_scores),\n","    \"BERTScore-F1\": F1.mean().item()\n","}"],"metadata":{"id":"aRbtWieczbVW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Print the results\n","print(\"Average Scores:\")\n","for metric, score in average_scores.items():\n","    print(f\"{metric}: {score:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xxl11PMizbSq","executionInfo":{"status":"ok","timestamp":1745737895173,"user_tz":-330,"elapsed":28,"user":{"displayName":"Anindita Bhattacharya","userId":"16794923920170422248"}},"outputId":"51389062-7f3f-4526-ee6c-8a884bd0b61d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Average Scores:\n","ROUGE-1: 0.1263\n","ROUGE-2: 0.0371\n","ROUGE-L: 0.1155\n","BERTScore-F1: 0.8111\n"]}]},{"cell_type":"code","source":["# Load metrics\n","meteor = evaluate.load(\"meteor\")"],"metadata":{"id":"EnVgIsH9hto8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculate METEOR\n","meteor_score = meteor.compute(predictions=predictions, references=references)"],"metadata":{"id":"kXJhTbiehtmg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Print METEOR score\n","print(\"METEOR:\", meteor_score)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m-WYJmV1htkC","executionInfo":{"status":"ok","timestamp":1745738089223,"user_tz":-330,"elapsed":23,"user":{"displayName":"Anindita Bhattacharya","userId":"16794923920170422248"}},"outputId":"8d7c6baa-56ac-4455-fecf-0ca37d17d22c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["METEOR: {'meteor': 0.10967740542194493}\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"ycOz3Se-hthQ"},"execution_count":null,"outputs":[]}]}